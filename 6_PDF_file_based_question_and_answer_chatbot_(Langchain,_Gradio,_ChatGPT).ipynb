{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaeyeon2367/llm-langchain-pdf-chabot/blob/main/6_PDF_file_based_question_and_answer_chatbot_(Langchain%2C_Gradio%2C_ChatGPT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF file-based question and answer chatbot (Langchain, Gradio, ChatGPT)"
      ],
      "metadata": {
        "id": "u-KIIvQ5NKJP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GHgrhVbLnM6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03517484-0cf5-4a8e-c0cc-b9aeefcbdbf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.4)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.5.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.14.3)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.339)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.66)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.5.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.14.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (2.2.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (6.0.1)\n",
            "Requirement already satisfied: loguru>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.8.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.26.18)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.104.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.1)\n",
            "Requirement already satisfied: gradio-client==0.7.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.19.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.5.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.8.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.0.post1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.13.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.14.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install pinecone-client\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
        "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\n",
        "os.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")"
      ],
      "metadata": {
        "id": "C5uYZQknXLDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09e0d12-cef6-4c6f-b06f-c25039b10ec2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n",
            "Pinecone API Key:··········\n",
            "Pinecone Environment:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "gveJwPgiVRMf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "V42-bhvZIisH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install chromadb\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSX_ndLWHJqy",
        "outputId": "3fb1d413-46c9-4522-9a22-dad9a7fa88d3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.17.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.17)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.5.1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.104.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.24.0.post1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.8.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.16.3)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.21.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.2)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (28.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.6.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3<2.0,>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.21.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.21.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.42b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.42b0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.14.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.1.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcAZD_ui_U9q",
        "outputId": "49fcf8d8-8691-47de-c9cc-89c904806d95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings.cohere import CohereEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.vectorstores import Pinecone"
      ],
      "metadata": {
        "id": "5kZxXmSjQRHy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "7VgBZ49vQVzg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hUyQu5tTdoJ",
        "outputId": "ef64871a-1d11-455a-cb2f-4439a9427d48"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='ChatGPT is not all you need. A State of the Art\\nReview of large Generative AI models\\nRoberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\nQuantitative Methods Department, Universidad Pontiﬁcia Comillas, Madrid, Spain\\n201905616@alu.comillas.edu, ecgarrido@icade.comillas.edu\\nAbstract. During the last two years there has been a plethora of large\\ngenerative models such as ChatGPT or Stable Diﬀusion that have been\\npublished. Concretely, these models are able to perform tasks such as\\nbeing a general question and answering system or automatically creat-\\ning artistic images that are revolutionizing several sectors. Consequently,\\nthe implications that these generative models have in the industry and\\nsociety are enormous, as several job positions may be transformed. For\\nexample, Generative AI is capable of transforming eﬀectively and cre-\\natively texts to images, like the DALLE-2 model; text to 3D images,\\nlike the Dreamfusion model; images to text, like the Flamingo model;\\ntexts to video, like the Phenaki model; texts to audio, like the AudioLM\\nmodel; texts to other texts, like ChatGPT; texts to code, like the Codex\\nmodel; texts to scientiﬁc texts, like the Galactica model or even create\\nalgorithms like AlphaTensor. This work consists on an attempt to de-\\nscribe in a concise way the main models are sectors that are aﬀected by\\ngenerative AI and to provide a taxonomy of the main generative models\\npublished recently.\\n1 Introduction\\nGenerative AI refers to artiﬁcial intelligence that can generate novel content,\\nrather than simply analyzing or acting on existing data like expert systems [23].\\nIn particular, expert systems contained knowledge bases and an inference engine\\nthat generated content via an if-else rule database. However, modern generative\\nartiﬁcial intelligence contain a discriminator or transformer model trained on a\\ncorpus or dataset that is able to map the input information into a latent high-\\ndimensional space and a generator model, that is able to generate an stochastic\\nbehaviour creating novel content in every new trial even from the same prompts\\nas an input, performing unsupervised, semi-supervised or supervised learning,\\ndepending on the particular methodology. Regarding the created content by the\\nmodel, generative artiﬁcial intelligence models are diﬀerent from predictive ma-\\nchine learning systems, that merely perform a discrimination behaviour, solv-\\ning classiﬁcation or regression problems. In particular, these models are able\\nto discriminate information and generate information of the transformed input\\ninformation, or prompt.\\nThe key aspect about generative models is that their architecture and the\\ndata that they have been fed is enormous. For example, it is possible now toarXiv:2301.04655v1  [cs.LG]  11 Jan 2023', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 0}),\n",
              " Document(page_content='2 Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\nestimate the parameters of the model by feeding it the contents of the whole\\nWikipedia, Github, social networks, Google images and more. Despite being fed\\nwith an enormous size of data, thanks to the rise of computing we can design\\ndeep neural networks [18], transformers [22] and other models such as genera-\\ntive adversarial networks [9] or variational autoencoders [23] whose capacity is\\nable to model the complexity of the data, without suﬀering from underﬁtting.\\nAs they are able to modelize the high-dimensional probability distribution of\\nlanguage or photos of a concrete or general domain, if they are complemented\\nby generative models that map the latent high-dimensional semantic space of\\nlanguage of photos to a multimedia representation of text, audio or video we\\ncan map any input format like texts to any output format like video. In this\\nsense, applications of this technology are endless, in the sense that we can train\\na model to generate genuine diﬀerent multimedia formats as video, audio or text\\nfrom diﬀerent multimedia input formats, as for example, text.\\nWe believe that it is necessary to provide a state-of-the-art review on the\\nmost popular generative AI models as they are revolutionizing several indus-\\ntries like the art industry [2] or universities [16,30]. As models are now able to\\ngenerate genuine artistic content or large texts answering a prompt, these two\\nindustries and other ones that we will detail throughout this manuscript will\\nneed to readapt their activity to continue providing value. In this sense, gen-\\nerative AI models will not replace humans but enhance our content, being an\\ninspiration for artists or improving the content generated by professors. In order\\nto provide information for a professional working in any industry that can be\\nbeneﬁted by these models we have made the organization of the paper as the\\nfollowing one. First, we will provide a taxonomy of the main generative models\\nthat have appeared in the industry. Then, the following sections will analyze\\neach of the categories of the taxonomy. Finally, we ﬁnish the manuscript with a\\nconclusions and further work section. We do not study the technical aspects of\\nevery model, such as transformers in detail as our purpose in this review is on the\\napplications of the models and the content that they generative but not on how\\nthey work. For a detailed explanation of deep learning models and generative\\nmodels we recommend other references [18,23].\\n2 A Taxonomy of Generative AI models\\nBefore analyzing each model in detail, we have tried to organize the current\\ngenerative artiﬁcial models into a taxonomy whose categories represent the main\\nmappings between each multimedia input and output type of data. The result\\nis the one that we have illustrated in Figure 1. We have discovered a total of 9\\ncategories, where each of the models that appear in Figure 1 will be described\\nin detail in the following section. Each of the covered models has been published\\nrecently, as we illustrate in Figure 2, as our main concern in this manuscript is\\nto describe the latest advances in generative AI models.\\nInterestingly, only six organizations are behind the deployment of these mod-\\nels, as we illustrate in Figure 3. The main reason behind this fact is that in order', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 1}),\n",
              " Document(page_content='State of the Art of Generative AI 3\\nFig. 1. A taxonomy of the most popular generative AI models that have recently\\nappeared classiﬁed according to their input and generated formats.\\nFig. 2. Covered models by date of release. All models were released during 2022 except\\nLaMDA, which was released in 2021 and Muse, in 2023.', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 2}),\n",
              " Document(page_content='4 Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\nto be able to estimate the parameters of these models, it is mandatory to have\\nan enormous computation power and a highly skilled and experienced team in\\ndata science and data engineering. Consequently, only the companies shown on\\nFigure 3, with the help of acquired startups and collaborations with academia,\\nhave been able to successfully deploy generative artiﬁcial intelligence models.\\nFig. 3. Models by developer. In terms of major companies participating in startups,\\nnote that Microsoft invested 1 billion dollars in OpenAI and helps them with the de-\\nvelopment of models. As well, note that Google acquired Deepmind in 2014. In terms\\nof universities, note that VisualGPT was developed by KAUST, Carnegie Mellon Uni-\\nversity and Nanyang Technological University and that the Human Motion Diﬀusion\\nModel was developed by Tel Aviv University, Israel. As well, other projects are de-\\nveloped by a company in collaboration with a university. Concretely, this is the case\\nfor Stable Diﬀsion (Runway, Stability AI and LMU MUNICH), Soundify (Runway and\\nCarnegie Mellon University) and DreamFusion (Google and UC Berkeley)', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 3}),\n",
              " Document(page_content='State of the Art of Generative AI 5\\nNow that we have provided and analyzed the latest generative artiﬁcial in-\\ntelligence models, the following section will cover each of the categories of the\\ntaxonomy presented in Figure 1 in detail.\\n3 Generative AI models categories\\nIn this section we will cover in detail the nine categories described in Figure 1 of\\nthe previous section. For every category, we illustrate the details of the models\\nshown in Figure 1.\\n3.1 Text-to-image models\\nWe begin the review by considering the models whose input is a text prompt\\nand whose output is an image.\\nDALL ·E 2: DALL ·E 2, created by OpenAI, is able to generate original, genuine\\nand realistic images and art from a prompt consisting on a text description [10].\\nLuckily, it is possible to use the OPENAI API to get access to this model. In\\nparticular, DALL ·E 2 manages to combine concepts, attributes and diﬀerent\\nstyles. In order to do so, it uses the CLIP neural network. CLIP (Contrastive\\nLanguage-Image Pre-Training) is a neural network trained on a variety of (image,\\ntext) pairs [25]. Using CLIP, that can be instructed in natural language to predict\\nthe most relevant text snippet, given an image, the model has recently merged\\nas a successful representation learner for images. Concretely, CLIP embeddings\\nhave several desirable properties: they are robust to image distribution shift, have\\nimpressive zero-shot capabilities and have been ﬁne-tuned to achieve state-of-the-\\nart results. In order to obtain a full generative model of images, the CLIP image\\nembedding decoder module is combined with a prior model, which generates\\npossible CLIP image embeddings from a given text caption. We illustrate an\\nimage generated from a prompt in Figure 4\\nFig. 4. Image generated from the prompt ”A shiba inu wearing a beret and black\\nturtleneck”.', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 4}),\n",
              " Document(page_content='6 Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\nIMAGEN : Imagen is a text-to-image diﬀusion model [17] consisting on large\\ntransformer language models. Critically, the main discovery observed with this\\nmodel made is that large language models, pre-trained on text-only corpora, are\\nvery eﬀective at encoding text for image synthesis [28]. Precisely, using Imagen,\\nit has been found out that increasing the size of the language model boosts both\\nsample ﬁdelity and image-text alignment much more than increasing the size of\\nthe image diﬀusion model. The model was created by Google and the API can\\nbe found in their web page. For the evaluation of their model, Google created\\nDrawbench, a set of 200 prompts that support the evaluation and comparison of\\ntext-to-image models. Most concretely, the model is based on a pretrained text\\nencoder (like BERT [12]) that performs a mapping from text to a sequence of\\nword embeddings and a cascade of conditional diﬀusion models that map these\\nembeddings to images of increasing resolutions. We show an image generated\\nfrom a prompt in Figure 5.\\nFig. 5. Image generated from the prompt ”A cute corgi lives in a house made out of\\nsushi”.\\nStable Diﬀusion : Stable Diﬀusion is a latent-diﬀusion model that is open-\\nsource and has been developed by the CompVis group at LMU Munich. The\\nmain diﬀerence of this model with respect to the other ones is the use of a\\nlatent diﬀusion model and that it performs image modiﬁcation as it can perform\\noperations in its latent space. For Stable Diﬀusion, we can use the API via their\\nwebsite. More concretely, Stable Diﬀusion consists of two parts: the text encoder\\nand the image generator [17]. The image information creator works completely\\nin the latent space. This property makes it faster than previous diﬀusion models\\nthat worked in a pixel space. We illustrate a Stable Diﬀusion image example in\\nFigure 7.', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 5}),\n",
              " Document(page_content='State of the Art of Generative AI 7\\nFig. 6. Image generated from the prompt ”A cute corgi lives in a house made out of\\nsushi”.\\nMuse : This model is a Text-to-image transformer model that achieves state-of-\\nthe-art image generation while being more eﬃcient than diﬀusion or autoregres-\\nsive models [6]. Concretely, it is trained on a masked modelling task in discrete\\ntoken space. Consequently, it is more eﬃcient because of the use of discrete\\ntokens and requiring fewer sampling iterations. Compared to Parti, a autore-\\ngressive model, Muse is more eﬃcient because of parallel decoding. Muse is 10x\\nfaster at inference time than Imagen-3B or Parti-3B and 3x faster than Stable\\nDiﬀusion v 1.4. Muse is also faster than than Stable Diﬀusion in spite of both\\nmodels working in the latent space of a VQGAN. We append a comparison of\\nthe generated images by DALL ·E 2, IMAGEN and Muse in Figure ??.\\n3.2 Text-to-3D models\\nThe models that have been described in the previous section deal with the map-\\nping of text prompts to 2D images. However, for some industries like gaming,\\nit is necessary to generate 3D images. In this section, we brieﬂy describe two\\ntext-to-3D models: Dreamfusion and Magic3D.\\nDreamfusion : DreamFusion is a text-to-3D model developed by Google Re-\\nsearch that uses a pretrained 2D text-to-image diﬀusion model to perform text-\\nto-3D synthesis [24]. In particular, Dreamfusion replaces previous CLIP tech-\\nniques with a loss derived from distillation of a 2D diﬀusion model. Concretely,\\nthe diﬀusion model can be used as a loss within a generic continuous optimization\\nproblem to generate samples. Critically, sampling in parameter space is much\\nharder than in pixels as we want to create 3D models that look like good images\\nwhen rendered from random angles. To solve the issue, this model uses a diﬀer-\\nentiable generator. Other approaches are focused on sampling pixels, however,\\nthis model instead focuses on creating 3D models that look like good images\\nwhen rendered from random angles. We illustrate in Figure 8 an example of', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 6}),\n",
              " Document(page_content='8 Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\nFig. 7. Comparison of generated images by the DALL ·E 2, IMAGEN and Muse models\\nwith respect to the prompts that appear in the column of the left. The ﬁrst column of\\nimages contains the results generated by DALL ·E 2, the second the results obtained\\nwith IMAGEN and the third the images created by Muse.\\nan image created by Dreamfusion from one particular angle along with all the\\nvariations that can be generated from additional text prompts. In order to see\\nthe full animated image, we recommend to visit the web page of Dreamfusion.\\nFig. 8. A 3D squirrel generated by Dreamfusion is shown at the left. Then, the other\\nimages contain the modiﬁcations generated to the squirrel with text prompts like ”wear-\\ning a jacket”.', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 7}),\n",
              " Document(page_content='State of the Art of Generative AI 9\\nMagic3D : This model is a text to 3D model made by NVIDIA Corporation.\\nWhile the Dreamfusion model achieves remarkable results, the method has two\\nproblems: mainly, the long processing time and the low-quality of the generated\\nimages. However, these problems are addressed by Magic3D using a two-stage\\noptimization framework [20]. Firstly, Magic3D builds a low-resolution diﬀusion\\nprior and, then, it accelerates with a sparse 3D hash grid structure. Using that,\\na textured 3D mesh model is furthered optimized with an eﬃcient diﬀerentiable\\nrender. Comparatively, regarding human evaluation, the model achieves better\\nresults, as 61.7% prefer this model to DreamFusion. As we can see in Figure 9,\\nMagic3D achieves much higher quality 3D shapes in both geometry and texture\\ncompared to DreamFusion.\\nFig. 9. 3D Images generated by Magic3D and Dreamfusion, where ”Ours” refer to\\nMagic3D. We can see a total of 8 text prompts and the images that both models\\ngenerate from that prompts.\\n3.3 Image-to-Text models\\nSometimes, it is also useful to obtain a text that describes an image, that is\\nprecisely the inverse mapping to the one that has been analyzed in the previous\\nsubsections. In this section, we analyze two models that perform this task, along\\nwith others: Flamingo and VisualGPT.\\nFlamingo : A Visual Language Model created by Deepmind using few shot\\nlearning on a wide range of open-ended vision and language tasks, simply by\\nbeing prompted with a few input/output examples [1]. Concretely, the input of\\nFlamingo contains visually conditioned autoregressive text generation models\\nable to ingest a sequence of text tokens interleaved with images and/or videos', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 8}),\n",
              " Document(page_content='10 Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\nand produce text as output. A query is made to the model along with a photo\\nor a video and the model answers with a text answer. Some examples can be\\nobserved in Figure 10. Flamingo models take advantage of two complementary\\nmodels: a vision model that analyzes visual scenes and a large language model\\nwhich performs a basic form of reasoning. The language model is trained on a\\nlarge amount of text data.\\nFig. 10. Input prompts that contain images and text and output generated text re-\\nspones from Flamingo. Every column contains a single example where we can see how\\nFlamingo answers the question using the image from the text.\\nVisualGPT : VisualGPT is an image captioning model made by OpenAI [7].\\nConcretely, VisualGPT leverages knowledge from the pretrained language model\\nGPT-2 [5]. In order to bridge the semantic gap between diﬀerent modalities, a\\nnovel encoder-decoder attention mechanism [33] is designed with an unsaturated\\nrectiﬁed gating function. Critically, the biggest advantage of this model is that\\nit does not need for as much data as other image-to-text models. In particular,\\nimproving data eﬃciency in image captioning networks would enable quick data\\ncuration, description of rare objects, and applications in specialized domains.\\nMost interestingly, the API of this model can be found on GitHub. We include\\nthree examples of text prompts generated by the model with respect to three\\nimages fed to the model in Figure 11.\\n3.4 Text-to-Video models\\nAs we have seen in the previous subsections, it is now possible to generate images\\nfrom text. Consequently, the next logical step is to generate videos, that are', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 9}),\n",
              " Document(page_content='State of the Art of Generative AI 11\\nFig. 11. Three examples of text prompts generated by the images shown on the left.\\nWe also show the attention scores that the model assign to every word of the texts.\\nIn the third image, we can see for example how the most discriminative information\\nabout the image is the word ”cat” and ”television”.\\nsequences of images, from texts. In this section, we provide information about\\ntwo models that are able to perform this task: Phenaki and Soundify.\\nPhenaki : This model has been made by Google Research, and it is capable\\nof performing realistic video synthesis, given a sequence of textual prompts [34].\\nMost interestingly, we can get access to the API of the model from GitHub. In\\nparticular, Phenaki is the ﬁrst model that can generate videos from open domain\\ntime variable prompts. To address data issues, it performs joint training on a\\nlarge image-text pairs dataset as well as a smaller number of video-text exam-\\nples can result in generalization beyond what is available in the video datasets.\\nThis is mainly due to image-text datasets having billions of inputs while text-', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 10}),\n",
              " Document(page_content='12 Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\nvideo datasets are much smaller. As well, limitations come from computational\\ncapabilities for videos of variable length.\\nThe model has three parts: the C-ViViT encoder, the training transformer\\nand the video generator. The encoder gets a compressed representation of videos.\\nFirst tokens are transformed into embeddings. This is followed by the temporal\\ntransformer, then the spatial transformer. After the output of the spatial trans-\\nformer, they apply a single linear projection without activation to map the tokens\\nback to pixel space. Consequently, the model generates temporally coherent and\\ndiverse videos conditioned on open domain prompts even when the prompt is a\\nnew composition of concepts. The videos can be minutes long, while the model\\nis trained on 1.4 second videos. Below we show in Figure 12 and in Figure 13\\nsome examples of the creation of a video through a series of text prompts and\\nfrom a series of text prompts and an image.\\nFig. 12. Sequence of images created by the Phenaki model given four diﬀerent prompts.', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 11}),\n",
              " Document(page_content='State of the Art of Generative AI 13\\nFig. 13. Sequences of images created by the Phenaki model given an image and the\\nprompt. We can see how the model is able to manipulate the given image according to\\nthe text prompt.\\nSoundify : In video editing, sound in half of the story. But, for professional\\nvideo editing, the problems come from ﬁnding suitable sounds, aligning sounds,\\nvideo and tuning parameters [21]. In order to solve this issue, Soundify is a\\nsystem developed by Runway that matches sound eﬀects to video. This system\\nuses quality sound eﬀects libraries and CLIP (a neural network with zero-shot\\nimage classiﬁcation capabilities cited before). Concretely, the system has three\\nparts: classiﬁcation, synchronization, and mix. The classiﬁcation matches eﬀects\\nto a video by classifying sound emitters within. To reduce the distinct sound\\nemitters, the video is split based on absolute color histogram distances. In the\\nsynchronization part, intervals are identiﬁed comparing eﬀects label with each\\nframe and pinpointing consecutive matches above a threshold. In the mix part,\\neﬀects are split into around one-second chunks. Critically, chunks are stitched\\nvia crossfades.\\n3.5 Text-to-Audio models\\nAs we have seen in the previous subsection, images are not the only important\\nnon-structured data format. For videos, for music and in lots of contexts, audio\\ncan be critical. Consequently, we analyze in this subsection three models whose\\ninput information is text and whose output information is audio.\\nAudioLM : This model has been made by Google for high-quality audio gener-\\nation with long-term consistency. In particular, AudioLM maps the input audio\\ninto a sequence of discrete tokens and casts audio generation as language mod-\\neling task in this representation space [4]. By training on large corpora of raw', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 12}),\n",
              " Document(page_content='14 Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\naudio waveforms, AudioLM learns to generate natural and coherent continua-\\ntions given short prompts. The approach can be extended beyond speech by\\ngenerating coherent piano music continuations, despite being trained without\\nany symbolic representation of music. As with the other models, the API can\\nbe found through GitHub. Audio signals involve multiple scales of abstractions.\\nWhen it comes to audio synthesis, multiple scales make achieving high audio\\nquality while displaying consistency very challenging. This gets achieved by this\\nmodel by combining recent advances in neural audio compression, self-supervised\\nrepresentation learning and language modelling.\\nIn terms of subjective evaluation, raters were asked to listen to a sample of\\n10 seconds and decide whether it is human speech or a synthetic continuation.\\nBased on 1000 ratings collected, the rate is 51.2%, which is not statistically\\nsigniﬁcant from assigning labels at random. This tells us that humans cannot\\ndiﬀerentiate between synthetic and real samples.\\nJukebox : This is a model, developed by OpenAI, that generates music with\\nsinging in the raw audio domain [13]. Once again, its API can be found in\\nGitHub. Previously, earlier models in the text-to-music genre generated music\\nsymbolically in the form of a pianoroll which speciﬁes timing, pitch and velocity.\\nThe challenging aspect is the non-symbolic approach where music is tried to be\\nproduced directly as a piece of audio. In fact, the space of raw audio is extremely\\nhigh dimensional which makes the problem very challenging. Consequently, the\\nkey issue is that modelling that raw audio produces long-range dependencies,\\nmaking it computationally challenging to learn the high-level semantics of music.\\nIn order to solve this issue, this model tries to solve it by means of a hi-\\nerarchical VQ-VAE architecture to compress audio into a discrete space [14],\\nwith a loss function designed to retain the most amount of information. This\\nmodel produces songs from very diﬀerent genres such as rock, hip-hop and jazz.\\nHowever, the model is just limited to English songs. Concretely, its dataset for\\ntraining is from 1.2 million songs from LyricWiki. The VQ-VAE has 5 billion\\nparameters and is trained on 9-second audio clips for 3 days.\\nWhisper : This model is an Audio-to-Text converter developed by OpenAI. It\\nachieves several tasks in this ﬁeld: multi-lingual speech recognition, translation\\nand language identiﬁcation [26]. As in previous cases, its API can be found\\nin the GitHub website. The goal of a speech recognition system should be to\\nwork reliably out of the box in a broad range of environments without requiring\\nsupervised ﬁne-tuning of a decoder for every deployment distribution. This is\\nhard because of the lack of a high-quality pre-trained decoder.\\nConcretely, this model is trained on 680,000 hours of labeled audio data.\\nThis data is collected from the internet, which results in a very diverse dataset\\ncovering a broad distribution of audio from many diﬀerent environments, record-\\nings setups, speakers and languages. The model makes sure that the dataset is\\nonly from human voice as machine learning voice would impair the model. Files', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 13}),\n",
              " Document(page_content='State of the Art of Generative AI 15\\nare broken in 30 second segments paired with the subset of the transcript that\\noccurs within that time segment.\\nThe model has an encoder-deccoder transformer, as this architecture has\\nbeen validated to scale reliably. We can observe the model’s architecture char-\\nacteristics through the ﬁgure below. We can see the diﬀerent types of data and\\nthe learning sequence.\\n3.6 Text-to-Text models\\nThe previous models all convert a non-structured data type into another one.\\nBut, regarding text, it is very useful to convert text into another text in order to\\nsatisfy tasks as general question and answering. The following four models treat\\ntext and also output texts to satisfy diﬀerent needs.\\nChatGPT : The popular ChatGPT is a model by OpenAI which interacts\\nin a conversational way. As it is widely known, the model answers follow-up\\nquestions, challenges incorrect premises and reject inappropriate requests. More\\nconcretely, the algorithm behind ChatGPT is based on a transformer. However,\\nthe training is made through Reinforcement Learning for Human Feedback. In\\nparticular, an initial model is trained using supervised ﬁne-tuning: human AI\\ntrainers would provide conversations in which they played both sides, the user\\nand an AI assistant. Then, those people would be given the model-written re-\\nsponses to help them compose their response. This dataset was mixed to that of\\nInstructGPT [3], which was transformed into a dialogue format. A demo can be\\nfound in their website and the API may also be found in OpenAI’s website. We\\nsummarize the main steps of ChatGPT training in Figure 14, available in the\\nChatGPT demo’s website. Finally, ChatGPT is also able to generate code and\\nsimple mathematics.\\nLaMDA : LaMDA is a language model for dialog applications [32]. Unlike\\nmost other language models, LaMDA was trained on dialogue. It is a family\\nof transformer-based neural language models specialized for dialog which have\\nup to 137B parameters and are pre-trained on 1.56T words of public dialog\\ndata and web text. Fine-tuning can enable for safety and factual grounding of\\nthe model. Only 0.001% of training data was used for ﬁne-tuning, which is a\\ngreat achievement of the model. In particular, dialog modes take advantage of\\nTransformers’ ability to present long-term dependencies in text. Concretely, they\\nare generally very well-suited for model scaling. Consequently, LaMDA makes\\nuse of a single model to perform multiple tasks: it generates several responses,\\nwhich are ﬁltered for safety, grounded on an external knowledge source and re-\\nranked to ﬁnd the highest-quality response. We illustrate in Figure 15 an example\\nof a dialog with the model.\\nPEER : Collaborative language model developed by Meta AI research trained\\non edit histories to cover the entire writing process [29]. It is based on four', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 14}),\n",
              " Document(page_content='16 Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\nFig. 14. Training steps of ChatGPT, combining supervised learning with reinforcement\\nlearning.\\nFig. 15. Example of a dialog made with LaMDA.\\nsteps: Plan, Edit, Explain and Repeat. These steps are repeated until the text\\nis in a satisfactory state that requires no further updates. The model allow to\\ndecompose the task of writing a paper into multiple easier subtasks. As well,\\nthe model allows humans to intervene at any time and steer the model in any\\ndirection.\\nIt is mainly trained on Wikipedia edit histories. The approach is a self-\\ntraining, using models to inﬁll missing data and then train other models on this\\nsynthetic data. The downside of this comes from comments being very noisy\\nand a lack of citations, which tries to be compensated by a retrieval system\\nwhich does not always work. The framework is based on an iterative process.', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 15}),\n",
              " Document(page_content='State of the Art of Generative AI 17\\nThe entire process of formulating a plan, collecting documents, performing an\\nedit and explaining it can be repeated multiple times until arriving at a sequence\\nof texts. For the training, a DeepSpeed transformer is used.\\nMeta AI Speech from Brain : Model developed by Meta AI to help people\\nunable to communicate through speech, typing or gestures [11]. Previous tech-\\nniques relied on invasive brain-recording techniques which require neurosurgical\\ninterventions. This model tries to decode language directly from noninvasive\\nbrain recordings. This would provide a safer, more scalable solution that could\\nbeneﬁt many more people. The challenge with this proposed method come from\\nnoise and diﬀerences in each person’s brain and where the sensors are placed.\\nA deep learning model is trained with contrastive learning and used to max-\\nimally align noninvasive brain recordings and speech sounds. A self-supervised\\nlearning model called wave2vec 2.0. is used to identify the complex representa-\\ntions of speech in the brains of volunteers listening to audiobooks. The two nonin-\\nvasive technologies used to measure neuronal activity are electroencephalography\\nand magnetoencephalography.\\nTraining data comes from four opensource datasets which represent 150 hours\\nof recordings of 169 volunteers listening to audiobooks. EEG and MEG record-\\nings are inserted into a brain model, which consists of a standard deep convolu-\\ntional network with residual connections. These recordings are what comes from\\nindividuals’ brains. This model then has both a speech model for sound and a\\nbrain model for MEG data.\\nResults show that several components of the algorithm were beneﬁcial to\\ndecoding performance. As well, analysis shows that the algorithm improves as\\nEEG and MEG recordings increase. This research shows that self-supervised\\ntrained AI can decode perveived speech despite noise and variability in that data.\\nThe biggest limitation of this research is that it focuses on speech perception,\\nbut the ultimate goal would be to extend this work to speech production.\\n3.7 Text-to-Code models\\nAlthough we have covered text-to-text models, not all texts follows the same\\nsyntax. An special type of text is code. In programming, it is essential to know\\nhow to convert an idea into code. In order to do so, Codex and Alphacode models\\nhelp.\\nCodex : AI system created by OpenAI which translates text to code. It is\\na general-purpose programming model, as it can be applied to basically any\\nprogramming task [8]. Programming can be broken down into two parts: breaking\\na problem down into simpler problems and mapping those problems into existing\\ncode (libraries, APIs, or functions) that already exist. The second part is the\\nmost time-barring part for programmers, and it is where Codex excels the most.\\nThe data collected for training was collected in May 2020 from public software\\nrepositories hosted on GitHub, containing 179GB of unique Python ﬁles under 1', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 16}),\n",
              " Document(page_content='18 Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\nMB. The model is ﬁne-tuned from GPT-3, which already contains strong natural\\nlanguage representations. The demo and the API can be found in Open AI’s\\nwebsite.\\nAlphacode : Other language models have demonstrated an impressive ability\\nto generate code, but these systems still perform poorly when evaluated on more\\ncomplex, unseen problems. However, Alphacode is a system for code generation\\nfor problems that require for deeper reasoning [19]. Three components are key for\\nthis achievement: having an extensive dataset for training and evaluation, large\\nand eﬃcient transformer based architectures and a large-scale model sampling.\\nIn terms of training, the model is ﬁrstly pre-trained through GitHub repos-\\nitories amounting to 715.1 GB of code. This is a much more extensive dataset\\nthan Codex’s pre training dataset. For the training to be better, a ﬁne-tuning\\ndataset is introduced from the Codeforces plataform. Through this platform,\\nCodecontests are conducted, for the validation phase, in which we better the per-\\nformance of the model. Regarding the transformer-based architecture, they use\\nan encoder-decoder transformer architecture. Compared to decoder-only archi-\\ntectures commonly used, this architecture allows for a bidirectional description\\nand extra ﬂexibility. As well, they use a shallow encoder and a deep encoder\\nto further the model’s eﬃciency. To reduce the cost of sampling, multi-query\\nattention is used.\\n3.8 Text-to-Science models\\nEven scientiﬁc texts are being targeted by generative AI, as the Galactica and\\nMinerva models have shown. Although there is a long way to manage success in\\nthis ﬁeld, it is critical to study the ﬁrst attempts towards automatic scientiﬁc\\ntext generation.\\nGalactica : Galactica is a new large model for automatically organizing science\\ndeveloped by Meta AI and Papers with Code. The main advantage of the model\\nis the ability to train on it for multiple epochs without overﬁtting, where up-\\nstream and downstream performance improves with use of repeated tokens. The\\ndataset design is critical to the approach as all of it is processed in a common\\nmarkdown format to blend knowledge between sources. Citations are processed\\nvia a certain token that allows researchers to predict a citation given any in-\\nput context. The capability of the model of predicting citations improves with\\nscale and the model becomes better at the distribution of citations. In addition,\\nthe model can perform multi-modal tasks involving SMILES chemical formulas\\nand protein sequences. Concretely, Galactica uses a transformer architecture in\\na decoder-only setup with GeLU activation for all model sizes.\\nMinerva : Language model capable of solving mathematical and scientiﬁc ques-\\ntions using step-by-step reasoning. Minerva has a very clear focus on the collec-\\ntion of training data for this purpose. It solves quantitative reasoning problems,', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 17}),\n",
              " Document(page_content='State of the Art of Generative AI 19\\nmakes models at scale and employs best-in-class inference techniques. Concretely,\\nMinerva solves these problems by generating solutions step-by-step, this means\\nincluding calculations and symbolic manipulation without having the need for\\nexternal tools such a calculator.\\n3.9 Other models\\nWe would like to ﬁnish our review by covering additional models that do not ﬁt\\nany of the categories mentioned previously.\\nAlphatensor, created by the research company Deepmind, is a completely\\nrevolutionary model in the industry for its ability to discover new algorithms\\n[15]. In the published example, Alpha Tensor creates a more eﬃcient algorithm\\nfor matrix multiplication, which is very important, as improving the eﬃciency\\nof algorithms aﬀects a lot of computations, from neural networks to scientiﬁc\\ncomputing routines.\\nThe methodology is based on a deep reinforcement learning approach in\\nwhich the agent, AlphaTensor is trained to play a single-player game where the\\nobjective is ﬁnding tensor decomposisitions within a ﬁnite factor space. At each\\nstep of the TensorGame, the player selects how to combine diﬀerent entries of the\\nmatrices to multiply. A score is assigned based on the number of selected oper-\\nations required to reach the correct multiplication result. To solve TensorGame,\\nan agent, AlphaTensor was developed. AlphaTensor uses a specialized neural\\nnetwork architecture to exploit symmetries using synthetic training games.\\nGATO is a single generalist agent made by Deepmind. It works as a multi-\\nmodal, multi-task, multi-embodiment generalist policy [27]. The same network\\nwith the same weights can carry very diﬀerent capabilities from playing Atari,\\ncaption images, chatting, stacking blocks and many more. There are many bene-\\nﬁts from using a single neural sequence model across all tasks. It reduces the need\\nfor hand crafting policy models with their own inductive biases. It increases the\\namount and diversity of training data. This general agent is successful at many\\ntasks and can be adapted with little extra data to succeed at an even larger\\nnumber of tasks. r training at the operating point of model scale that allows\\nreal-time control of real-world robots, currently around 1.2B parameters in the\\ncase of GATO.\\nOther published generative AI models are able to generate human motion\\n[31] or, in the case of ChatBCG, slides using ChatGPT as a surrogate model.\\n4 Conclusions and further work\\nThrough this paper, we can observe the capabilities which generative artiﬁcial\\nintelligence has. We have seen a great deal of creativity as well as personalization\\nin tasks such as text-to-image or in tasks such as text-to-audio. They also are\\naccurate in text-to-science or text-to-code tasks. This can help economies in a\\nmajor way as it can help optimize creative and non-creative tasks.', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 18}),\n",
              " Document(page_content='20 Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\nHowever, because of the way that they are constructed at the moment, these\\nmodels face a number of limitations. In terms of dataset, ﬁnding data for some\\nof the models found such as the text-to-science or the text-to-audio is very hard,\\nmaking it very time-consuming to train the model. In particular, datasets and\\nparameters have to be enormous, making it harder to train. One of the biggest\\nissues with models is trying solutions out of the problems in the dataset, with\\nwhich models have more trouble solving. As well, in terms of computation, a lot\\nof time and computation capacity is necessary in order to run them. Many days\\nand advanced computers are needed in order to run the models.\\nIn addition, these models face bias from the data which needs to be controlled.\\nGalactica model tries to control this issue through a layer of no bias, but it still\\na major issue for Generative Artiﬁcial Intelligence.\\nWith the Minerva model, we can see that the model knows the steps which it\\nneeds to take to solve an equation. This is groundbreaking as one of the biggest\\nlimitations with these models is that the models do not understand exactly\\nwhat they are doing. Moreover, it’s still an industry starting; thus accuracy is\\nstill an issue. Text-to-video models for example are only represented by Phenaki\\nbecause how hard it is to produce accurate videos. Text-to-science models do\\nﬁnd some accuracy but that accuracy is still way behind to what it should be\\nfor professionals to actually rely on this technology on a day-to-day basis.\\nFurthermore, these models need to be constrained because of a lack of un-\\nderstanding of ethics. Phenaki on its paper even acknowledges that a system\\nlike text-to-video can be used to create deep-fakes. Lastly, we are still in a phase\\nwhere we are discovering what exactly the purpose of this intelligence will be.\\nThere has been articles comparing Google to ChatGPT3, which is totally inexact\\nas ChatGPT3 does not update its information in real time. We should be aware\\nabout the limitations of these models to try and improve them in the following\\nyears.\\nReferences\\n1.Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y.,\\nLenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual\\nlanguage model for few-shot learning. arXiv preprint arXiv:2204.14198 (2022).\\n2.Anantrasirichai, N., and Bull, D. Artiﬁcial intelligence in the creative indus-\\ntries: a review. Artiﬁcial Intelligence Review (2021), 1–68.\\n3.Bhavya, B., Xiong, J., and Zhai, C. Analogy generation by prompting large\\nlanguage models: A case study of instructgpt. arXiv preprint arXiv:2210.04186\\n(2022).\\n4.Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O.,\\nSharifi, M., Teboul, O., Grangier, D., Tagliasacchi, M., and Zeghidour,\\nN.Audiolm: a language modeling approach to audio generation. arXiv preprint\\narXiv:2209.03143 (2022).\\n5.Budzianowski, P., and Vuli ´c, I. Hello, it’s gpt-2–how can i help you? towards\\nthe use of pretrained language models for task-oriented dialogue systems. arXiv\\npreprint arXiv:1907.05774 (2019).', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 19}),\n",
              " Document(page_content='State of the Art of Generative AI 21\\n6.Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L.,\\nYang, M.-H., Murphy, K., Freeman, W. T., Rubinstein, M., et al. Muse:\\nText-to-image generation via masked generative transformers. arXiv preprint\\narXiv:2301.00704 (2023).\\n7.Chen, J., Guo, H., Yi, K., Li, B., and Elhoseiny, M. Visualgpt: Data-eﬃcient\\nadaptation of pretrained language models for image captioning. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (2022),\\npp. 18030–18040.\\n8.Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J.,\\nEdwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large\\nlanguage models trained on code. arXiv preprint arXiv:2107.03374 (2021).\\n9.Creswell, A., White, T., Dumoulin, V., Arulkumaran, K., Sengupta, B.,\\nand Bharath, A. A. Generative adversarial networks: An overview. IEEE signal\\nprocessing magazine 35 , 1 (2018), 53–65.\\n10.Daras, G., and Dimakis, A. G. Discovering the hidden vocabulary of dalle-2.\\narXiv preprint arXiv:2206.00169 (2022).\\n11.D´efossez, A., Caucheteux, C., Rapin, J., Kabeli, O., and King, J.-R. De-\\ncoding speech from non-invasive brain recordings. arXiv preprint arXiv:2208.12266\\n(2022).\\n12.Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training\\nof deep bidirectional transformers for language understanding. arXiv preprint\\narXiv:1810.04805 (2018).\\n13.Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., and Sutskever,\\nI.Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341 (2020).\\n14.Ding, S., and Gutierrez-Osuna, R. Group latent embedding for vector quan-\\ntized variational autoencoder in non-parallel voice conversion. In INTERSPEECH\\n(2019), pp. 724–728.\\n15.Fawzi, A., Balog, M., Huang, A., Hubert, T., Romera-Paredes, B.,\\nBarekatain, M., Novikov, A., R Ruiz, F. J., Schrittwieser, J., Swirszcz,\\nG., et al. Discovering faster matrix multiplication algorithms with reinforcement\\nlearning. Nature 610 , 7930 (2022), 47–53.\\n16.Kandlhofer, M., Steinbauer, G., Hirschmugl-Gaisch, S., and Huber, P.\\nArtiﬁcial intelligence and computer science in education: From kindergarten to\\nuniversity. In 2016 IEEE Frontiers in Education Conference (FIE) (2016), IEEE,\\npp. 1–9.\\n17.Kingma, D., Salimans, T., Poole, B., and Ho, J. Variational diﬀusion models.\\nAdvances in neural information processing systems 34 (2021), 21696–21707.\\n18.LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. nature 521 , 7553 (2015),\\n436–444.\\n19.Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R.,\\nEccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. Competition-level\\ncode generation with alphacode. Science 378 , 6624 (2022), 1092–1097.\\n20.Lin, C.-H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis,\\nK., Fidler, S., Liu, M.-Y., and Lin, T.-Y. Magic3d: High-resolution text-to-3d\\ncontent creation. arXiv preprint arXiv:2211.10440 (2022).\\n21.Lin, D. C.-E., Germanidis, A., Valenzuela, C., Shi, Y., and Martelaro,\\nN.Soundify: Matching sound eﬀects to video. arXiv preprint arXiv:2112.09726\\n(2021).\\n22.Lin, T., Wang, Y., Liu, X., and Qiu, X. A survey of transformers. AI Open\\n(2022).', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 20}),\n",
              " Document(page_content='22 Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\n23.Murphy, K. P. Probabilistic machine learning: an introduction . MIT press, 2022.\\n24.Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dreamfusion: Text-\\nto-3d using 2d diﬀusion. arXiv preprint arXiv:2209.14988 (2022).\\n25.Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,\\nSastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable\\nvisual models from natural language supervision. In International Conference on\\nMachine Learning (2021), PMLR, pp. 8748–8763.\\n26.Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and\\nSutskever, I. Robust speech recognition via large-scale weak supervision. arXiv\\npreprint arXiv:2212.04356 (2022).\\n27.Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A.,\\nBarth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T.,\\net al. A generalist agent. arXiv preprint arXiv:2205.06175 (2022).\\n28.Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.,\\nGhasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., et al.\\nPhotorealistic text-to-image diﬀusion models with deep language understanding.\\narXiv preprint arXiv:2205.11487 (2022).\\n29.Schick, T., Dwivedi-Yu, J., Jiang, Z., Petroni, F., Lewis, P., Izacard, G.,\\nYou, Q., Nalmpantis, C., Grave, E., and Riedel, S. Peer: A collaborative\\nlanguage model. arXiv preprint arXiv:2208.11663 (2022).\\n30.Susnjak, T. Chatgpt: The end of online exam integrity? arXiv preprint\\narXiv:2212.09292 (2022).\\n31.Tevet, G., Raab, S., Gordon, B., Shafir, Y., Cohen-Or, D., and Bermano,\\nA. H. Human motion diﬀusion model. arXiv preprint arXiv:2209.14916 (2022).\\n32.Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A.,\\nCheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al. Lamda: Language\\nmodels for dialog applications. arXiv preprint arXiv:2201.08239 (2022).\\n33.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\\nA. N., Kaiser,  L., and Polosukhin, I. Attention is all you need. Advances in\\nneural information processing systems 30 (2017).\\n34.Villegas, R., Babaeizadeh, M., Kindermans, P.-J., Moraldo, H., Zhang,\\nH., Saffar, M. T., Castro, S., Kunze, J., and Erhan, D. Phenaki: Variable\\nlength video generation from open domain textual description. arXiv preprint\\narXiv:2210.02399 (2022).', metadata={'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'page': 21})]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "\n",
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"PINECONE_API_KEY\"),  # find at app.pinecone.io\n",
        "    environment=os.getenv(\"PINECONE_ENV\"),  # next to api key in console\n",
        ")\n",
        "\n",
        "index_name = \"langchainfundamentals\"\n",
        "\n",
        "# First, check if our index already exists. If it doesn't, we create it\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # we create a new index\n",
        "    pinecone.create_index(name=index_name, metric=\"cosine\", dimension=1536)\n",
        "# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\n",
        "docsearch = Pinecone.from_documents(texts, embeddings, index_name=index_name)\n",
        "\n",
        "# if you already have an index, you can load it like this\n",
        "# docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
        "\n",
        "query = \"What's a Generative AI?\"\n",
        "docs = docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "tK2Hy9glCYMI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pinecone.Index(\"langchainfundamentals\")\n",
        "vectorstore = Pinecone(index, embeddings.embed_query, \"text\")\n",
        "\n",
        "vectorstore.add_texts(\"More text!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCIWO9CxDYvU",
        "outputId": "fbe7e708-e91e-42d8-eff6-e3063a453d63"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/pinecone.py:59: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['b213b0ac-8a08-469f-9eb4-a422940088a7',\n",
              " '27128319-dcf2-4490-ae02-a8d864db07f5',\n",
              " 'f4550b16-c88c-4b78-b56a-e962b714beb1',\n",
              " '8a341183-a5b1-4300-bcbb-985607133de3',\n",
              " 'd49f2cb6-694e-4528-99fb-1d5c3c39ba13',\n",
              " '8b8b4067-414d-46e2-8b2d-7007b7fa44a3',\n",
              " '1d3593a2-ec55-419f-b5f6-90f191895162',\n",
              " '466fa811-4508-405e-a5b5-36a1ebc064f3',\n",
              " '945437cf-bd29-448f-a41f-8f50a1d97ac2',\n",
              " '097e8f3a-260e-4233-8509-62b4cf4d632c']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = Chroma.from_documents(texts, embeddings)\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "4otdIuNkT5b_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)  # Modify model_name if you have access to GPT-4\n",
        "\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever = retriever,\n",
        "    return_source_documents=True)"
      ],
      "metadata": {
        "id": "gY8Mtw-OamW8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's a Generative AI?\"\n",
        "result = chain(query)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "xDW1hZX3a_1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd59f44-1075-4453-99e0-b3155f6dca42"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': \"What's a Generative AI?\", 'answer': \"La Generative AI (Intelligence Artificielle Générative) fait référence à l'intelligence artificielle capable de générer du contenu nouveau, plutôt que d'analyser ou d'agir sur des données existantes comme le font les systèmes experts. Ces modèles d'intelligence artificielle générative sont capables de créer du contenu original en utilisant des modèles de discrimination ou de transformation entraînés sur de vastes ensembles de données. Ils peuvent générer des textes, des images, des vidéos, du code informatique, des textes scientifiques, etc. en fonction de leur architecture et des données sur lesquelles ils ont été formés. Ces modèles ont des implications énormes dans l'industrie et la société, car ils peuvent transformer de nombreux secteurs et métiers. [SOURCE]\", 'sources': '', 'source_documents': [Document(page_content='ChatGPT is not all you need. A State of the Art\\nReview of large Generative AI models\\nRoberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\nQuantitative Methods Department, Universidad Pontiﬁcia Comillas, Madrid, Spain\\n201905616@alu.comillas.edu, ecgarrido@icade.comillas.edu\\nAbstract. During the last two years there has been a plethora of large\\ngenerative models such as ChatGPT or Stable Diﬀusion that have been\\npublished. Concretely, these models are able to perform tasks such as\\nbeing a general question and answering system or automatically creat-\\ning artistic images that are revolutionizing several sectors. Consequently,\\nthe implications that these generative models have in the industry and\\nsociety are enormous, as several job positions may be transformed. For\\nexample, Generative AI is capable of transforming eﬀectively and cre-\\natively texts to images, like the DALLE-2 model; text to 3D images,\\nlike the Dreamfusion model; images to text, like the Flamingo model;\\ntexts to video, like the Phenaki model; texts to audio, like the AudioLM\\nmodel; texts to other texts, like ChatGPT; texts to code, like the Codex\\nmodel; texts to scientiﬁc texts, like the Galactica model or even create\\nalgorithms like AlphaTensor. This work consists on an attempt to de-\\nscribe in a concise way the main models are sectors that are aﬀected by\\ngenerative AI and to provide a taxonomy of the main generative models\\npublished recently.\\n1 Introduction\\nGenerative AI refers to artiﬁcial intelligence that can generate novel content,\\nrather than simply analyzing or acting on existing data like expert systems [23].\\nIn particular, expert systems contained knowledge bases and an inference engine\\nthat generated content via an if-else rule database. However, modern generative\\nartiﬁcial intelligence contain a discriminator or transformer model trained on a\\ncorpus or dataset that is able to map the input information into a latent high-\\ndimensional space and a generator model, that is able to generate an stochastic\\nbehaviour creating novel content in every new trial even from the same prompts\\nas an input, performing unsupervised, semi-supervised or supervised learning,\\ndepending on the particular methodology. Regarding the created content by the\\nmodel, generative artiﬁcial intelligence models are diﬀerent from predictive ma-\\nchine learning systems, that merely perform a discrimination behaviour, solv-\\ning classiﬁcation or regression problems. In particular, these models are able\\nto discriminate information and generate information of the transformed input\\ninformation, or prompt.\\nThe key aspect about generative models is that their architecture and the\\ndata that they have been fed is enormous. For example, it is possible now toarXiv:2301.04655v1  [cs.LG]  11 Jan 2023', metadata={'page': 0, 'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf'}), Document(page_content='State of the Art of Generative AI 3\\nFig. 1. A taxonomy of the most popular generative AI models that have recently\\nappeared classiﬁed according to their input and generated formats.\\nFig. 2. Covered models by date of release. All models were released during 2022 except\\nLaMDA, which was released in 2021 and Muse, in 2023.', metadata={'page': 2, 'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf'})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "Qr0_iNsLTb6y",
        "outputId": "e3030a1c-92d3-4a21-b707-a574aeb6d013"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"La Generative AI (Intelligence Artificielle Générative) fait référence à l'intelligence artificielle capable de générer du contenu nouveau, plutôt que d'analyser ou d'agir sur des données existantes comme le font les systèmes experts. Ces modèles d'intelligence artificielle générative sont capables de créer du contenu original en utilisant des modèles de discrimination ou de transformation entraînés sur de vastes ensembles de données. Ils peuvent générer des textes, des images, des vidéos, du code informatique, des textes scientifiques, etc. en fonction de leur architecture et des données sur lesquelles ils ont été formés. Ces modèles ont des implications énormes dans l'industrie et la société, car ils peuvent transformer de nombreux secteurs et métiers. [SOURCE]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['sources']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Yjs65WFeTivP",
        "outputId": "9d4ac256-9a89-4c8e-82fe-c222faf82190"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "system_template=\"\"\"Use the following pieces of context to answer the users question shortly.\n",
        "Given the following summaries of a long document and a question, create a final answer with references (\"SOURCES\"), use \"SOURCES\" in capital letters regardless of the number of sources.\n",
        "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
        "----------------\n",
        "{summaries}\n",
        "\n",
        "You MUST answer in French and in Markdown format:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "]\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(messages)"
      ],
      "metadata": {
        "id": "InBq74JFacah"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)  # Modify model_name if you have access to GPT-4\n",
        "\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever = retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "GfUMi5ClS_4O"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Quel est le concept de Generative AI?\"\n",
        "result = chain(query)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "pVdJrxN0TzwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "cM3DcAF4Uqvh",
        "outputId": "0854e5e0-6b18-4e9a-8fa7-65d8ecf34b87"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"La Generative AI fait référence à l'intelligence artificielle capable de générer du contenu nouveau et original, plutôt que d'analyser ou d'agir sur des données existantes. Ces modèles utilisent des architectures complexes et sont entraînés sur de vastes ensembles de données pour créer du contenu de manière autonome. Ils peuvent générer des images, du texte, de la musique, des vidéos, du code, etc. La Generative AI a des implications importantes dans de nombreux secteurs et peut transformer certains emplois. [SOURCE]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['source_documents']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxNF1I2pUzIv",
        "outputId": "75edafce-d2ad-42a9-ddb0-54fed2d11fc0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='State of the Art of Generative AI 3\\nFig. 1. A taxonomy of the most popular generative AI models that have recently\\nappeared classiﬁed according to their input and generated formats.\\nFig. 2. Covered models by date of release. All models were released during 2022 except\\nLaMDA, which was released in 2021 and Muse, in 2023.', metadata={'page': 2, 'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'text': 'State of the Art of Generative AI 3\\nFig. 1. A taxonomy of the most popular generative AI models that have recently\\nappeared classiﬁed according to their input and generated formats.\\nFig. 2. Covered models by date of release. All models were released during 2022 except\\nLaMDA, which was released in 2021 and Muse, in 2023.'}),\n",
              " Document(page_content='ChatGPT is not all you need. A State of the Art\\nReview of large Generative AI models\\nRoberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\nQuantitative Methods Department, Universidad Pontiﬁcia Comillas, Madrid, Spain\\n201905616@alu.comillas.edu, ecgarrido@icade.comillas.edu\\nAbstract. During the last two years there has been a plethora of large\\ngenerative models such as ChatGPT or Stable Diﬀusion that have been\\npublished. Concretely, these models are able to perform tasks such as\\nbeing a general question and answering system or automatically creat-\\ning artistic images that are revolutionizing several sectors. Consequently,\\nthe implications that these generative models have in the industry and\\nsociety are enormous, as several job positions may be transformed. For\\nexample, Generative AI is capable of transforming eﬀectively and cre-\\natively texts to images, like the DALLE-2 model; text to 3D images,\\nlike the Dreamfusion model; images to text, like the Flamingo model;\\ntexts to video, like the Phenaki model; texts to audio, like the AudioLM\\nmodel; texts to other texts, like ChatGPT; texts to code, like the Codex\\nmodel; texts to scientiﬁc texts, like the Galactica model or even create\\nalgorithms like AlphaTensor. This work consists on an attempt to de-\\nscribe in a concise way the main models are sectors that are aﬀected by\\ngenerative AI and to provide a taxonomy of the main generative models\\npublished recently.\\n1 Introduction\\nGenerative AI refers to artiﬁcial intelligence that can generate novel content,\\nrather than simply analyzing or acting on existing data like expert systems [23].\\nIn particular, expert systems contained knowledge bases and an inference engine\\nthat generated content via an if-else rule database. However, modern generative\\nartiﬁcial intelligence contain a discriminator or transformer model trained on a\\ncorpus or dataset that is able to map the input information into a latent high-\\ndimensional space and a generator model, that is able to generate an stochastic\\nbehaviour creating novel content in every new trial even from the same prompts\\nas an input, performing unsupervised, semi-supervised or supervised learning,\\ndepending on the particular methodology. Regarding the created content by the\\nmodel, generative artiﬁcial intelligence models are diﬀerent from predictive ma-\\nchine learning systems, that merely perform a discrimination behaviour, solv-\\ning classiﬁcation or regression problems. In particular, these models are able\\nto discriminate information and generate information of the transformed input\\ninformation, or prompt.\\nThe key aspect about generative models is that their architecture and the\\ndata that they have been fed is enormous. For example, it is possible now toarXiv:2301.04655v1  [cs.LG]  11 Jan 2023', metadata={'page': 0, 'source': '/content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf', 'text': 'ChatGPT is not all you need. A State of the Art\\nReview of large Generative AI models\\nRoberto Gozalo-Brizuela, Eduardo C. Garrido-Merch´ an\\nQuantitative Methods Department, Universidad Pontiﬁcia Comillas, Madrid, Spain\\n201905616@alu.comillas.edu, ecgarrido@icade.comillas.edu\\nAbstract. During the last two years there has been a plethora of large\\ngenerative models such as ChatGPT or Stable Diﬀusion that have been\\npublished. Concretely, these models are able to perform tasks such as\\nbeing a general question and answering system or automatically creat-\\ning artistic images that are revolutionizing several sectors. Consequently,\\nthe implications that these generative models have in the industry and\\nsociety are enormous, as several job positions may be transformed. For\\nexample, Generative AI is capable of transforming eﬀectively and cre-\\natively texts to images, like the DALLE-2 model; text to 3D images,\\nlike the Dreamfusion model; images to text, like the Flamingo model;\\ntexts to video, like the Phenaki model; texts to audio, like the AudioLM\\nmodel; texts to other texts, like ChatGPT; texts to code, like the Codex\\nmodel; texts to scientiﬁc texts, like the Galactica model or even create\\nalgorithms like AlphaTensor. This work consists on an attempt to de-\\nscribe in a concise way the main models are sectors that are aﬀected by\\ngenerative AI and to provide a taxonomy of the main generative models\\npublished recently.\\n1 Introduction\\nGenerative AI refers to artiﬁcial intelligence that can generate novel content,\\nrather than simply analyzing or acting on existing data like expert systems [23].\\nIn particular, expert systems contained knowledge bases and an inference engine\\nthat generated content via an if-else rule database. However, modern generative\\nartiﬁcial intelligence contain a discriminator or transformer model trained on a\\ncorpus or dataset that is able to map the input information into a latent high-\\ndimensional space and a generator model, that is able to generate an stochastic\\nbehaviour creating novel content in every new trial even from the same prompts\\nas an input, performing unsupervised, semi-supervised or supervised learning,\\ndepending on the particular methodology. Regarding the created content by the\\nmodel, generative artiﬁcial intelligence models are diﬀerent from predictive ma-\\nchine learning systems, that merely perform a discrimination behaviour, solv-\\ning classiﬁcation or regression problems. In particular, these models are able\\nto discriminate information and generate information of the transformed input\\ninformation, or prompt.\\nThe key aspect about generative models is that their architecture and the\\ndata that they have been fed is enormous. For example, it is possible now toarXiv:2301.04655v1  [cs.LG]  11 Jan 2023'})]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in result['source_documents']:\n",
        "    print('Content : ' + doc.page_content[0:100].replace('\\n', ' '))\n",
        "    print('File : ' + doc.metadata['source'])\n",
        "    print('Page : ' + str(doc.metadata['page']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSOqIBzXU6f_",
        "outputId": "ef98c4b6-a86d-4192-f00d-6fc7854742a1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content : State of the Art of Generative AI 3 Fig. 1. A taxonomy of the most popular generative AI models that\n",
            "File : /content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf\n",
            "Page : 2\n",
            "Content : ChatGPT is not all you need. A State of the Art Review of large Generative AI models Roberto Gozalo-\n",
            "File : /content/drive/MyDrive/data-llm/A State of the Art Review of large Generative AI models.pdf\n",
            "Page : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def respond(message, chat_history):  # Define a function to handle responses from the chatbot.\n",
        "\n",
        "    result = chain(message)\n",
        "\n",
        "    bot_message = result['answer']\n",
        "\n",
        "    # Append source document information to the bot's response\n",
        "    for i, doc in enumerate(result['source_documents']):\n",
        "        bot_message += '[' + str(i+1) + '] ' + doc.metadata['source'] + '(' + str(doc.metadata['page']) + ') '\n",
        "\n",
        "    chat_history.append((message, bot_message))  # Add the user's message and the bot's response to the chat history.\n",
        "\n",
        "    return \"\", chat_history  # Return the modified chat history.\n",
        "\n",
        "with gr.Blocks() as demo:  # Create an interface using gr.Blocks().\n",
        "    chatbot = gr.Chatbot(label=\"Chat Window\")  # Create a chatbot component with the label 'Chat Window'.\n",
        "    msg = gr.Textbox(label=\"Input\")  # Create a textbox with the label 'Input'.\n",
        "    clear = gr.Button(\"Clear\")  # Create a button with the label 'Clear'.\n",
        "\n",
        "    # When a message is entered and submitted in the textbox, call the respond function.\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "    # When the 'Clear' button is clicked, reset the chat history.\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.launch(debug=True)  # Run the interface. Upon execution, users can write messages in the 'Input' textbox, submit them, and clear the chat history using the 'Clear' button."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "Yl_SXA37i4p5",
        "outputId": "58874309-99c7-4b7f-b2f8-e6fe24707693"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://16837d11390d1ce188.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://16837d11390d1ce188.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://16837d11390d1ce188.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}